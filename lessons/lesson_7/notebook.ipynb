{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7 - Pandas\n",
    "\n",
    "In this lesson we will get to know the **Pandas** library. We start with the basics—importing the library and creating simple Series and DataFrames—and move to more advanced topics like merging, pivot tables, multi-indexing, and time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Getting Started with Pandas\n",
    "\n",
    "This chapter introduces the basics of Pandas. We'll learn how to import the library, create a Series, build a DataFrame from a dictionary, and read data from a CSV file—all using interesting, real-world inspired datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing Pandas and Checking the Version\n",
    "\n",
    "Before using Pandas, we need to import it. We also check the version to ensure it's correctly installed. This simple step confirms that your Python environment is set up for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating a Pandas Series\n",
    "\n",
    "A Pandas Series is a one-dimensional labeled array capable of holding any data type. In this example, we create a Series from a Python list. This is one of the most fundamental data structures in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Temperatures (°C):\n",
      "Mon    20\n",
      "Tue    21\n",
      "Wed    22\n",
      "Thu    22\n",
      "Fri    20\n",
      "Sat    23\n",
      "Sun    24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a Pandas Series for daily temperatures in °C\n",
    "temperatures = [20, 21, 22, 22, 20, 23, 24]  # Temperatures in Celsius\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "temp_series = pd.Series(temperatures, index=days)\n",
    "print(\"Daily Temperatures (°C):\")\n",
    "print(temp_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating a DataFrame from a Dictionary\n",
    "\n",
    "A DataFrame is a two-dimensional data structure that resembles a table (like a spreadsheet or SQL table). In this example, we construct a DataFrame using a Python dictionary, where keys become column names and the values become the corresponding column data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Inventory DataFrame:\n",
      "      Product  Price  Quantity     Category\n",
      "0      Laptop   1200        10  Electronics\n",
      "1  Smartphone    800        20  Electronics\n",
      "2      Tablet    450        15  Electronics\n",
      "3  Headphones    150        50  Accessories\n",
      "4  Smartwatch    200        30  Electronics\n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame from a dictionary with product data\n",
    "product_data = {\n",
    "    'Product': ['Laptop', 'Smartphone', 'Tablet', 'Headphones', 'Smartwatch'],\n",
    "    'Price': [1200, 800, 450, 150, 200],\n",
    "    'Quantity': [10, 20, 15, 50, 30],\n",
    "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Accessories', 'Electronics']\n",
    "}\n",
    "df_products = pd.DataFrame(product_data)\n",
    "print(\"Product Inventory DataFrame:\")\n",
    "print(df_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Reading Data from a CSV File\n",
    "\n",
    "Often, your data will be stored in external files such as CSV files. Pandas makes it easy to read these files into a DataFrame. Make sure your CSV file (e.g., `data.csv`) is in the same directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created from a CSV file:\n",
      "     Duration  Pulse  Maxpulse  Calories\n",
      "0          60    110       130     409.1\n",
      "1          60    117       145     479.0\n",
      "2          60    103       135     340.0\n",
      "3          45    109       175     282.4\n",
      "4          45    117       148     406.0\n",
      "..        ...    ...       ...       ...\n",
      "164        60    105       140     290.8\n",
      "165        60    110       145     300.0\n",
      "166        60    115       145     310.2\n",
      "167        75    120       150     320.4\n",
      "168        75    125       150     330.4\n",
      "\n",
      "[169 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines if you have a CSV file named \"data.csv\"\n",
    "df_csv = pd.read_csv(\"data.csv\")\n",
    "print(\"DataFrame created from a CSV file:\")\n",
    "print(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Manipulation and Analysis (Intermediate)\n",
    "\n",
    "In this chapter, we explore data manipulation techniques using a dataset of employee information. We will inspect the data, perform filtering and selection, use indexing methods, sort the data, add/drop columns, and finally, group and aggregate the data for summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Viewing and Inspecting Data\n",
    "\n",
    "After loading data, it’s important to inspect it. Methods like `.head()`, `.info()`, and `.describe()` help you quickly understand the structure, data types, and summary statistics of your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the DataFrame:\n",
      "   Duration  Pulse  Maxpulse  Calories\n",
      "0        60    110       130     409.1\n",
      "1        60    117       145     479.0\n",
      "2        60    103       135     340.0\n",
      "3        45    109       175     282.4\n",
      "4        45    117       148     406.0\n",
      "\n",
      "DataFrame Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 169 entries, 0 to 168\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Duration  169 non-null    int64  \n",
      " 1   Pulse     169 non-null    int64  \n",
      " 2   Maxpulse  169 non-null    int64  \n",
      " 3   Calories  164 non-null    float64\n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 5.4 KB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "         Duration       Pulse    Maxpulse     Calories\n",
      "count  169.000000  169.000000  169.000000   164.000000\n",
      "mean    63.846154  107.461538  134.047337   375.790244\n",
      "std     42.299949   14.510259   16.450434   266.379919\n",
      "min     15.000000   80.000000  100.000000    50.300000\n",
      "25%     45.000000  100.000000  124.000000   250.925000\n",
      "50%     60.000000  105.000000  131.000000   318.600000\n",
      "75%     60.000000  111.000000  141.000000   387.600000\n",
      "max    300.000000  159.000000  184.000000  1860.400000\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 rows of the DataFrame:\")\n",
    "print(df_csv.head())\n",
    "\n",
    "print(\"\\nDataFrame Information:\")\n",
    "print(df_csv.info())\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df_csv.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Selecting and Filtering Data\n",
    "\n",
    "Data selection and filtering allow you to focus on subsets of your data that meet certain criteria. In this example, we:\n",
    "- Select a single column.\n",
    "- Filter rows based on a condition (e.g., employees with a salary above \\$55,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Names:\n",
      "0      Alice\n",
      "1        Bob\n",
      "2    Charlie\n",
      "3      David\n",
      "4        Eva\n",
      "5      Frank\n",
      "Name: Employee, dtype: object\n",
      "\n",
      "Employees with Salary > 55000:\n",
      "  Employee  Age Department  Salary\n",
      "1      Bob   34         IT   60000\n",
      "3    David   42         IT   70000\n",
      "5    Frank   36    Finance   58000\n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame with employee data\n",
    "employee_data = {\n",
    "    'Employee': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank'],\n",
    "    'Age': [28, 34, 29, 42, 30, 36],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR', 'Finance'],\n",
    "    'Salary': [50000, 60000, 55000, 70000, 52000, 58000]\n",
    "}\n",
    "df_employees = pd.DataFrame(employee_data)\n",
    "\n",
    "# Selecting the 'Employee' column\n",
    "employee_names = df_employees['Employee']\n",
    "print(\"Employee Names:\")\n",
    "print(employee_names)\n",
    "\n",
    "# Filtering rows where Salary > 55000\n",
    "high_salary = df_employees[df_employees['Salary'] > 55000]\n",
    "print(\"\\nEmployees with Salary > 55000:\")\n",
    "print(high_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Using loc and iloc for Indexing\n",
    "\n",
    "Pandas provides two main indexing methods: `loc` for label-based indexing and `iloc` for integer-based indexing. These methods allow precise selection of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department of the second employee (using loc): IT\n",
      "Age of the third employee (using iloc): 29\n"
     ]
    }
   ],
   "source": [
    "# Using loc: Retrieve the department of the second employee (index 1)\n",
    "dept_second_employee = df_employees.loc[1, 'Department']\n",
    "print(\"Department of the second employee (using loc):\", dept_second_employee)\n",
    "\n",
    "# Using iloc: Retrieve the age of the third employee (position 2)\n",
    "age_third_employee = df_employees.iloc[2, 1]\n",
    "print(\"Age of the third employee (using iloc):\", age_third_employee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Sorting DataFrames\n",
    "\n",
    "Sorting data is key to data analysis. Here, we sort the DataFrame by `Age` (ascending order) and then by `Department` and `Salary` (with Salary in descending order) to demonstrate how you can organize your data for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees sorted by Age:\n",
      "  Employee  Age Department  Salary\n",
      "0    Alice   28         HR   50000\n",
      "2  Charlie   29    Finance   55000\n",
      "4      Eva   30         HR   52000\n",
      "1      Bob   34         IT   60000\n",
      "5    Frank   36    Finance   58000\n",
      "3    David   42         IT   70000\n",
      "\n",
      "Employees sorted by Department and Salary (Salary descending):\n",
      "  Employee  Age Department  Salary\n",
      "5    Frank   36    Finance   58000\n",
      "2  Charlie   29    Finance   55000\n",
      "4      Eva   30         HR   52000\n",
      "0    Alice   28         HR   50000\n",
      "3    David   42         IT   70000\n",
      "1      Bob   34         IT   60000\n"
     ]
    }
   ],
   "source": [
    "# Sorting by Age (ascending)\n",
    "sorted_by_age = df_employees.sort_values(by='Age')\n",
    "print(\"Employees sorted by Age:\")\n",
    "print(sorted_by_age)\n",
    "\n",
    "# Sorting by Department and then by Salary (with Salary in descending order)\n",
    "sorted_by_dept_salary = df_employees.sort_values(by=['Department', 'Salary'], ascending=[True, False])\n",
    "print(\"\\nEmployees sorted by Department and Salary (Salary descending):\")\n",
    "print(sorted_by_dept_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Adding and Dropping Columns\n",
    "\n",
    "DataFrames are mutable, meaning you can easily add or remove columns as needed. In this example, we add a new column called `Bonus` and later drop it from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after adding Bonus column:\n",
      "  Employee  Age Department  Salary  Bonus\n",
      "0    Alice   28         HR   50000   5000\n",
      "1      Bob   34         IT   60000   7000\n",
      "2  Charlie   29    Finance   55000   6000\n",
      "3    David   42         IT   70000   8000\n",
      "4      Eva   30         HR   52000   5500\n",
      "5    Frank   36    Finance   58000   6500\n",
      "\n",
      "DataFrame after dropping Bonus column:\n",
      "  Employee  Age Department  Salary\n",
      "0    Alice   28         HR   50000\n",
      "1      Bob   34         IT   60000\n",
      "2  Charlie   29    Finance   55000\n",
      "3    David   42         IT   70000\n",
      "4      Eva   30         HR   52000\n",
      "5    Frank   36    Finance   58000\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column 'Bonus'\n",
    "df_employees['Bonus'] = [5000, 7000, 6000, 8000, 5500, 6500]\n",
    "print(\"DataFrame after adding Bonus column:\")\n",
    "print(df_employees)\n",
    "\n",
    "# Dropping the 'Bonus' column\n",
    "# The parameter inplace=True modifies the DataFrame in place\n",
    "# If inplace=False, the drop operation will return a new DataFrame\n",
    "df_employees.drop('Bonus', axis=1, inplace=True)\n",
    "print(\"\\nDataFrame after dropping Bonus column:\")\n",
    "print(df_employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Grouping and Aggregating Data\n",
    "\n",
    "Grouping data allows you to perform aggregate operations on subsets of data. In this example, we group the data by `City` and compute the mean of the `Age` column for each group. This is a powerful way to summarize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Employee Data (mean Salary and Age by Department):\n",
      "             Salary   Age\n",
      "Department               \n",
      "Finance     56500.0  32.5\n",
      "HR          51000.0  29.0\n",
      "IT          65000.0  38.0\n"
     ]
    }
   ],
   "source": [
    "# Grouping by 'Department' and calculating the mean Salary and Age\n",
    "grouped_employees = df_employees.groupby('Department').agg({'Salary': 'mean', 'Age': 'mean'})\n",
    "print(\"Grouped Employee Data (mean Salary and Age by Department):\")\n",
    "print(grouped_employees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Advanced Topics (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Merging and Joining DataFrames\n",
    "\n",
    "In this example, we have two datasets:\n",
    "\n",
    "1. **Employee Data:** Contains details about employees including a unique `EmployeeID`, `Name`, `Age`, and `DepartmentID`.\n",
    "2. **Department Data:** Contains department details including `DepartmentID`, `DepartmentName`, and `Manager`.\n",
    "\n",
    "The two DataFrames share a common key: `DepartmentID`. We'll merge these DataFrames using different join types:\n",
    "- **Inner Join:** Only includes rows with matching `DepartmentID` in both DataFrames.\n",
    "- **Left Join:** Keeps all employee records and adds department information where available.\n",
    "- **Outer Join:** Combines all records from both DataFrames, showing all employees and departments even if there’s no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Data:\n",
      "   EmployeeID     Name  Age  DepartmentID\n",
      "0         101    Alice   25             1\n",
      "1         102      Bob   30             2\n",
      "2         103  Charlie   35             1\n",
      "3         104    David   40             3\n",
      "4         105      Eva   28             2\n",
      "5         106    Frank   32             4\n",
      "\n",
      "Department Data:\n",
      "   DepartmentID DepartmentName    Manager\n",
      "0             1             HR       Anne\n",
      "1             2             IT      Brian\n",
      "2             3        Finance  Catherine\n",
      "3             5      Marketing      Derek\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Employee data with additional details\n",
    "employee_data = {\n",
    "    'EmployeeID': [101, 102, 103, 104, 105, 106],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank'],\n",
    "    'Age': [25, 30, 35, 40, 28, 32],\n",
    "    'DepartmentID': [1, 2, 1, 3, 2, 4]  # Note: DepartmentID 4 has no matching department record below\n",
    "}\n",
    "df_employee = pd.DataFrame(employee_data)\n",
    "print(\"Employee Data:\")\n",
    "print(df_employee)\n",
    "\n",
    "# Department data with additional departments\n",
    "department_data = {\n",
    "    'DepartmentID': [1, 2, 3, 5],  # Note: DepartmentID 5 does not match any employee\n",
    "    'DepartmentName': ['HR', 'IT', 'Finance', 'Marketing'],\n",
    "    'Manager': ['Anne', 'Brian', 'Catherine', 'Derek']\n",
    "}\n",
    "df_department = pd.DataFrame(department_data)\n",
    "print(\"\\nDepartment Data:\")\n",
    "print(df_department)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Inner Join\n",
    "\n",
    "Only includes rows with matching `DepartmentID` in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inner Merge Result (only matching DepartmentID):\n",
      "   EmployeeID     Name  Age  DepartmentID DepartmentName    Manager\n",
      "0         101    Alice   25             1             HR       Anne\n",
      "1         102      Bob   30             2             IT      Brian\n",
      "2         103  Charlie   35             1             HR       Anne\n",
      "3         104    David   40             3        Finance  Catherine\n",
      "4         105      Eva   28             2             IT      Brian\n"
     ]
    }
   ],
   "source": [
    "merged_inner = pd.merge(df_employee, df_department, on='DepartmentID', how='inner')\n",
    "print(\"\\nInner Merge Result (only matching DepartmentID):\")\n",
    "print(merged_inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Left Join\n",
    "\n",
    "Keeps all employee records and adds department information where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Left Merge Result (all employees, with department info if available):\n",
      "   EmployeeID     Name  Age  DepartmentID DepartmentName    Manager\n",
      "0         101    Alice   25             1             HR       Anne\n",
      "1         102      Bob   30             2             IT      Brian\n",
      "2         103  Charlie   35             1             HR       Anne\n",
      "3         104    David   40             3        Finance  Catherine\n",
      "4         105      Eva   28             2             IT      Brian\n",
      "5         106    Frank   32             4            NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "merged_left = pd.merge(df_employee, df_department, on='DepartmentID', how='left')\n",
    "print(\"\\nLeft Merge Result (all employees, with department info if available):\")\n",
    "print(merged_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Outer Join\n",
    "\n",
    "Combines all records from both DataFrames, showing all employees and departments even if there’s no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer Merge Result (all records from both DataFrames):\n",
      "   EmployeeID     Name   Age  DepartmentID DepartmentName    Manager\n",
      "0       101.0    Alice  25.0             1             HR       Anne\n",
      "1       103.0  Charlie  35.0             1             HR       Anne\n",
      "2       102.0      Bob  30.0             2             IT      Brian\n",
      "3       105.0      Eva  28.0             2             IT      Brian\n",
      "4       104.0    David  40.0             3        Finance  Catherine\n",
      "5       106.0    Frank  32.0             4            NaN        NaN\n",
      "6         NaN      NaN   NaN             5      Marketing      Derek\n"
     ]
    }
   ],
   "source": [
    "merged_outer = pd.merge(df_employee, df_department, on='DepartmentID', how='outer')\n",
    "print(\"\\nOuter Merge Result (all records from both DataFrames):\")\n",
    "print(merged_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating Pivot Tables\n",
    "\n",
    "In this example, we create a dataset with 12 rows containing the following columns:\n",
    "- **Date:** The date of the record.\n",
    "- **Region:** The geographical region (e.g., East or West).\n",
    "- **Category:** A categorical variable (e.g., A or B).\n",
    "- **Sales:** Numeric sales figures.\n",
    "- **Profit:** Numeric profit figures.\n",
    "\n",
    "We then create two pivot tables:\n",
    "1. **Total Sales by Date and Category (with Region as columns):** This pivot table groups the data by Date and Category, showing the total Sales for each Region.\n",
    "2. **Average Profit by Region and Category:** This pivot table shows the average Profit for each combination of Region and Category.\n",
    "\n",
    "These examples illustrate how pivot tables can be used to quickly summarize and analyze multidimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "          Date Region Category  Sales  Profit\n",
      "0   2025-01-01   East        A    100      20\n",
      "1   2025-01-01   East        B    150      30\n",
      "2   2025-01-01   West        A    200      40\n",
      "3   2025-01-01   West        B    250      50\n",
      "4   2025-01-02   East        A    110      22\n",
      "5   2025-01-02   East        B    160      32\n",
      "6   2025-01-02   West        A    210      42\n",
      "7   2025-01-02   West        B    260      52\n",
      "8   2025-01-03   East        A    120      24\n",
      "9   2025-01-03   East        B    170      34\n",
      "10  2025-01-03   West        A    220      44\n",
      "11  2025-01-03   West        B    270      54\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset for the pivot table\n",
    "data_pivot = {\n",
    "    'Date': ['2025-01-01'] * 4 + ['2025-01-02'] * 4 + ['2025-01-03'] * 4,\n",
    "    'Region': ['East', 'East', 'West', 'West'] * 3,\n",
    "    'Category': ['A', 'B', 'A', 'B'] * 3,\n",
    "    'Sales': [100, 150, 200, 250, 110, 160, 210, 260, 120, 170, 220, 270],\n",
    "    'Profit': [20, 30, 40, 50, 22, 32, 42, 52, 24, 34, 44, 54]\n",
    "}\n",
    "df_pivot = pd.DataFrame(data_pivot)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot Table: Total Sales by Date and Category (columns = Region)\n",
      "Region               East  West\n",
      "Date       Category            \n",
      "2025-01-01 A          100   200\n",
      "           B          150   250\n",
      "2025-01-02 A          110   210\n",
      "           B          160   260\n",
      "2025-01-03 A          120   220\n",
      "           B          170   270\n"
     ]
    }
   ],
   "source": [
    "# Pivot Table 1: Total Sales by Date and Category, with Region as columns\n",
    "pivot_table_sales = pd.pivot_table(\n",
    "    df_pivot,\n",
    "    values='Sales',\n",
    "    index=['Date', 'Category'],\n",
    "    columns='Region',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "print(\"\\nPivot Table: Total Sales by Date and Category (columns = Region)\")\n",
    "print(pivot_table_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot Table: Average Profit by Region and Category\n",
      "Category     A     B\n",
      "Region              \n",
      "East      22.0  32.0\n",
      "West      42.0  52.0\n"
     ]
    }
   ],
   "source": [
    "# Pivot Table 2: Average Profit by Region and Category\n",
    "pivot_table_profit = pd.pivot_table(\n",
    "    df_pivot,\n",
    "    values='Profit',\n",
    "    index='Region',\n",
    "    columns='Category',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nPivot Table: Average Profit by Region and Category\")\n",
    "print(pivot_table_profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Indexing\n",
    "\n",
    "In this example, we create a dataset representing different stores in various regions and cities.  \n",
    "- **Region:** The geographical region (e.g., North, South, East, West).  \n",
    "- **City:** The city where the store is located.  \n",
    "- **Store:** The name of the store.  \n",
    "- **Sales:** The sales figures for each store.  \n",
    "- **Employees:** The number of employees at the store.\n",
    "\n",
    "We then set a multi-index on the DataFrame using the 'Region' and 'City' columns. This allows you to organize and access your data in a hierarchical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Region           City    Store  Sales  Employees\n",
      "0  North       New York  Store A    250         10\n",
      "1  North         Boston  Store B    300         15\n",
      "2  South        Atlanta  Store C    150          8\n",
      "3  South          Miami  Store D    400         20\n",
      "4   East   Philadelphia  Store E    320         12\n",
      "5   East      Baltimore  Store F    210          9\n",
      "6   West    Los Angeles  Store G    500         18\n",
      "7   West  San Francisco  Store H    450         16\n",
      "\n",
      "DataFrame with Multi-Index (Region, City):\n",
      "                        Store  Sales  Employees\n",
      "Region City                                    \n",
      "North  New York       Store A    250         10\n",
      "       Boston         Store B    300         15\n",
      "South  Atlanta        Store C    150          8\n",
      "       Miami          Store D    400         20\n",
      "East   Philadelphia   Store E    320         12\n",
      "       Baltimore      Store F    210          9\n",
      "West   Los Angeles    Store G    500         18\n",
      "       San Francisco  Store H    450         16\n",
      "\n",
      "Data for South region:\n",
      "           Store  Sales  Employees\n",
      "City                              \n",
      "Atlanta  Store C    150          8\n",
      "Miami    Store D    400         20\n"
     ]
    }
   ],
   "source": [
    "# Enhanced dataset for multi-indexing\n",
    "data_multi = {\n",
    "    'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],\n",
    "    'City': ['New York', 'Boston', 'Atlanta', 'Miami', 'Philadelphia', 'Baltimore', 'Los Angeles', 'San Francisco'],\n",
    "    'Store': ['Store A', 'Store B', 'Store C', 'Store D', 'Store E', 'Store F', 'Store G', 'Store H'],\n",
    "    'Sales': [250, 300, 150, 400, 320, 210, 500, 450],\n",
    "    'Employees': [10, 15, 8, 20, 12, 9, 18, 16]\n",
    "}\n",
    "df_multi = pd.DataFrame(data_multi)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_multi)\n",
    "\n",
    "# Setting a multi-index using 'Region' and 'City'\n",
    "df_multi_indexed = df_multi.set_index(['Region', 'City'])\n",
    "print(\"\\nDataFrame with Multi-Index (Region, City):\")\n",
    "print(df_multi_indexed)\n",
    "\n",
    "# Accessing data: Get all stores in the 'South' region\n",
    "print(\"\\nData for South region:\")\n",
    "print(df_multi_indexed.loc['South'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Working with Time Series Data\n",
    "\n",
    "Here, we simulate a time series dataset representing daily store performance over 10 days.  \n",
    "- **Date:** The date is used as the index.  \n",
    "- **Sales:** Daily sales figures.  \n",
    "- **Visitors:** The number of visitors each day.\n",
    "\n",
    "We then demonstrate how to resample the data to different frequencies—for instance, calculating the average sales and visitors over a 3-day period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series DataFrame:\n",
      "            Sales  Visitors\n",
      "2025-01-01    200        50\n",
      "2025-01-02    220        55\n",
      "2025-01-03    250        60\n",
      "2025-01-04    270        65\n",
      "2025-01-05    300        70\n",
      "2025-01-06    310        75\n",
      "2025-01-07    290        72\n",
      "2025-01-08    280        68\n",
      "2025-01-09    260        66\n",
      "2025-01-10    300        74\n",
      "\n",
      "Resampled Time Series DataFrame (3-day average):\n",
      "                 Sales   Visitors\n",
      "2025-01-01  223.333333  55.000000\n",
      "2025-01-04  293.333333  70.000000\n",
      "2025-01-07  276.666667  68.666667\n",
      "2025-01-10  300.000000  74.000000\n"
     ]
    }
   ],
   "source": [
    "# Creating a date range for 10 days\n",
    "dates = pd.date_range(start='2025-01-01', periods=10, freq='D')\n",
    "\n",
    "# Simulated time series data for daily sales and visitors\n",
    "data_time = {\n",
    "    'Sales': [200, 220, 250, 270, 300, 310, 290, 280, 260, 300],\n",
    "    'Visitors': [50, 55, 60, 65, 70, 75, 72, 68, 66, 74]\n",
    "}\n",
    "df_time = pd.DataFrame(data_time, index=dates)\n",
    "print(\"Time Series DataFrame:\")\n",
    "print(df_time)\n",
    "\n",
    "# Resample the time series data: Compute the mean every 3 days\n",
    "df_resampled = df_time.resample('3D').mean()\n",
    "print(\"\\nResampled Time Series DataFrame (3-day average):\")\n",
    "print(df_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Advanced Aggregation with Custom Functions\n",
    "\n",
    "This example uses a dataset of store sales across various regions.  \n",
    "- **Region:** The region where the store is located.  \n",
    "- **Store:** The store identifier.  \n",
    "- **Sales:** The sales figures.\n",
    "\n",
    "We then group the data by 'Region' and apply a custom aggregation function that calculates the range (i.e., the difference between the maximum and minimum sales) within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Aggregation DataFrame:\n",
      "   Region Store  Sales\n",
      "0   North     A    120\n",
      "1   North     B    300\n",
      "2   North     C    250\n",
      "3   South     D    230\n",
      "4   South     E    400\n",
      "5   South     F    350\n",
      "6    East     G    150\n",
      "7    East     H    320\n",
      "8    East     I    280\n",
      "9    West     J    210\n",
      "10   West     K    450\n",
      "11   West     L    390\n",
      "\n",
      "Custom Aggregation (Sales range) grouped by Region:\n",
      "        Sales\n",
      "Region       \n",
      "East      170\n",
      "North     180\n",
      "South     170\n",
      "West      240\n"
     ]
    }
   ],
   "source": [
    "# Enhanced dataset for custom aggregation\n",
    "data_custom = {\n",
    "    'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East', 'West', 'West', 'West'],\n",
    "    'Store': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'],\n",
    "    'Sales': [120, 300, 250, 230, 400, 350, 150, 320, 280, 210, 450, 390]\n",
    "}\n",
    "df_custom = pd.DataFrame(data_custom)\n",
    "print(\"Custom Aggregation DataFrame:\")\n",
    "print(df_custom)\n",
    "\n",
    "# Define a custom function to compute the range of sales\n",
    "def range_func(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "# Group by 'Region' and apply the custom aggregation function on 'Sales'\n",
    "agg_custom = df_custom.groupby('Region').agg({'Sales': range_func})\n",
    "print(\"\\nCustom Aggregation (Sales range) grouped by Region:\")\n",
    "print(agg_custom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
